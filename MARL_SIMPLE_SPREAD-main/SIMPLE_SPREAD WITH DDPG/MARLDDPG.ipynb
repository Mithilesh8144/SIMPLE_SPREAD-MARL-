{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pettingzoo\n",
    "from pettingzoo.mpe import simple_spread_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MITHILESH\\.conda\\envs\\tensorflow\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env =simple_spread_v1.parallel_env(max_frames=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upper_bound = 1\n",
    "lower_bound = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, buffer_capacity=5, batch_size=3):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer1 = np.zeros((self.buffer_capacity, 18))\n",
    "        self.action_buffer1 = np.zeros((self.buffer_capacity, 2))\n",
    "        self.next_state_buffer1=np.zeros((self.buffer_capacity,18))\n",
    "        self.reward_buffer1=np.zeros((self.buffer_capacity,1))\n",
    "        \n",
    "        self.state_buffer2 = np.zeros((self.buffer_capacity, 18))\n",
    "        self.action_buffer2 = np.zeros((self.buffer_capacity, 2))\n",
    "        self.next_state_buffer2=np.zeros((self.buffer_capacity,18))\n",
    "        self.reward_buffer2=np.zeros((self.buffer_capacity,1))\n",
    "        \n",
    "        self.state_buffer3 = np.zeros((self.buffer_capacity, 18))\n",
    "        self.action_buffer3 = np.zeros((self.buffer_capacity, 2))\n",
    "        self.next_state_buffer3=np.zeros((self.buffer_capacity,18))\n",
    "        self.reward_buffer3=np.zeros((self.buffer_capacity,1))\n",
    "        \n",
    "        self.state_batches={'agent_0':0,'agent_1':2,'agent_2':0}\n",
    "        self.action_batches={'agent_0':0,'agent_1':0,'agent_2':0}\n",
    "        self.next_state_batches={'agent_0':0,'agent_1':0,'agent_2':0}\n",
    "        self.reward_batches={'agent_0':0,'agent_1':0,'agent_2':0}\n",
    "        \n",
    "        self.state_buffers={'agent_0':self.state_buffer1,'agent_1':self.state_buffer2,'agent_2':self.state_buffer3}\n",
    "        self.action_buffers={'agent_0':self.action_buffer1,'agent_1':self.action_buffer2,'agent_2':self.action_buffer3}\n",
    "        self.next_state_buffers={'agent_0':self.next_state_buffer1,'agent_1':self.next_state_buffer2,'agent_2':self.next_state_buffer3}\n",
    "        self.reward_buffers={'agent_0':self.reward_buffer1,'agent_1':self.reward_buffer2,'agent_2':self.reward_buffer3}\n",
    "        \n",
    "        self.target_actor1=get_actor()\n",
    "        self.target_actor2=get_actor()\n",
    "        self.target_actor3=get_actor()\n",
    "        self.target_actor_models={'agent_0':self.target_actor1,'agent_1':self.target_actor2,'agent_2':self.target_actor3}\n",
    "        \n",
    "        self.target_critic = get_critic()\n",
    "        self.critic=get_critic()\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        self.actor1=get_actor()\n",
    "        self.actor2=get_actor()\n",
    "        self.actor3=get_actor()\n",
    "        self.actor_models={'agent_0':self.actor1,'agent_1':self.actor2,'agent_2':self.actor3}\n",
    "        \n",
    "        self.critic_lr = 0.002\n",
    "        self.actor_lr = 0.001\n",
    "        self.gamma=0.99\n",
    "        self.tau=0.005\n",
    "        self.epsilon=1\n",
    "        self.episode_steps=0\n",
    "        self.sum_rewards = {'agent_0':0,'agent_1':0,'agent_2':0} \n",
    "        self.upper_bound=1\n",
    "        self.lower_bound=-1\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_lr)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_lr)\n",
    "        \n",
    "        for agent in env.agents:\n",
    "            self.target_actor_models[agent].set_weights(self.actor_models[agent].get_weights())\n",
    "    \n",
    "    \n",
    "    \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        for agent in env.agents:\n",
    "            self.state_buffers[agent][index]=obs_tuple[0][agent]\n",
    "            self.action_buffers[agent][index]=obs_tuple[1][agent]\n",
    "            self.reward_buffers[agent][index]=obs_tuple[2][agent]\n",
    "            self.next_state_buffers[agent][index]=obs_tuple[3][agent]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        for agent in self.state_batches:\n",
    "            self.state_batches[agent]=tf.convert_to_tensor(self.state_buffers[agent][batch_indices],dtype=tf.float32)\n",
    "            self.action_batches[agent]=tf.convert_to_tensor(self.action_buffers[agent][batch_indices],dtype=tf.float32)\n",
    "            self.next_state_batches[agent]=tf.convert_to_tensor(self.next_state_buffers[agent][batch_indices],dtype=tf.float32)\n",
    "            self.reward_batches[agent]=tf.convert_to_tensor(self.reward_buffers[agent][batch_indices],dtype=tf.float32)\n",
    "            \n",
    "        self.update()\n",
    "    def update(self):\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        next_states=[]\n",
    "        rewards=[]\n",
    "        target_actions=[]\n",
    "        action_batch=[]\n",
    "        for agent in env.agents:\n",
    "            states.append(self.state_batches[agent])\n",
    "            actions.append(self.action_batches[agent])\n",
    "            next_states.append(self.next_state_batches[agent])\n",
    "            rewards.append(self.reward_batches[agent])\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            for agent in env.agents:\n",
    "                actions_=self.target_actor_models[agent](self.next_state_batches[agent], training=True)\n",
    "                actions_=tf.reshape(actions_,(self.batch_size,-1))\n",
    "                target_actions.append(actions_)\n",
    "            y = rewards + self.gamma * self.target_critic([next_states, target_actions], training=True)\n",
    "            critic_value = self.critic([states, actions], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "            critic_grad = tape.gradient(critic_loss,self.critic.trainable_variables)\n",
    "            self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad,self.critic.trainable_variables)\n",
    "        )\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            for agent in env.agents:\n",
    "                action_batch.append(self.actor_models[agent](self.state_batches[agent], training=True))\n",
    "            \n",
    "            critic_value = self.critic([states, action_batch], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "            \n",
    "            for agent in env.agents:\n",
    "                actor_grad = tape.gradient(actor_loss,self.actor_models[agent].trainable_variables)\n",
    "                self.actor_optimizer.apply_gradients(zip(actor_grad,self.actor_models[agent].trainable_variables))\n",
    "    \n",
    "                                    \n",
    "                                    \n",
    "    def policy(self,agent,state,noise_object):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.actor_models[agent](state))\n",
    "        noise = noise_object()\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "        sampled_actions=np.clip(sampled_actions,self.lower_bound,self.upper_bound)\n",
    "        return sampled_actions\n",
    "    \n",
    "    @tf.function\n",
    "    def update_target(self,target_weights, weights):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic():\n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    state_input1=layers.Input(shape=(18))\n",
    "    state_input2=layers.Input(shape=(18))\n",
    "    state_input3=layers.Input(shape=(18))\n",
    "    state_input=layers.Concatenate()([state_input1,state_input2,state_input3])\n",
    "    state_output=layers.Dense(32,activation='relu')(state_input)\n",
    "    \n",
    "    \n",
    "    action_input=layers.Input(shape=(2))\n",
    "    \n",
    "    action_input1=layers.Input(shape=(2))\n",
    "    action_input2=layers.Input(shape=(2))\n",
    "    action_input3=layers.Input(shape=(2))\n",
    "    action_input=layers.Concatenate()([action_input1,action_input2,action_input3])\n",
    "    action_output=layers.Dense(32,activation='relu')(action_input)\n",
    "\n",
    "    concat=layers.Concatenate()([state_output,action_output])\n",
    "    x=layers.Dense(256,activation='relu')(concat)\n",
    "    \n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(1,activation='linear')(x)\n",
    "    model=tf.keras.Model([state_input1,state_input2,state_input3,action_input1,action_input2,action_input3],x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    last_init=tf.random_uniform_initializer(minval=-0.003,maxval=0.003)\n",
    "    i=layers.Input(shape=(18))\n",
    "    x=layers.Dense(256,activation='relu')(i)\n",
    "    x=layers.Dense(256,activation='relu')(x)\n",
    "    x=layers.Dense(2,activation='tanh')(x)\n",
    "    model=tf.keras.Model(i,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1=Agent(50000,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE  0  STEPS =  102\n",
      "REWARD  {'agent_0': -3439.9526617900597, 'agent_1': -3440.4526617900597, 'agent_2': -3439.9526617900597}\n",
      "EPISODE  1  STEPS =  102\n",
      "REWARD  {'agent_0': -2037.0331480191853, 'agent_1': -2037.0331480191853, 'agent_2': -2037.0331480191853}\n",
      "EPISODE  2  STEPS =  102\n",
      "REWARD  {'agent_0': -2062.8346428814857, 'agent_1': -2062.8346428814857, 'agent_2': -2062.8346428814857}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6f6935cb3b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0magent1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0magent1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-53b3c9cae70e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-53b3c9cae70e>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mactor_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1629\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5605\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5606\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5607\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5608\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_reward_list = []\n",
    "\n",
    "episode_steps=[]\n",
    "\n",
    "for episode in range(1000):\n",
    "\n",
    "    last_states = env.reset()\n",
    "    agent1.sum_rewards = {'agent_0':0,'agent_1':0,'agent_2':0}\n",
    "    agent1.episode_steps=0\n",
    "    while True:\n",
    "   \n",
    "        actions = {agent: agent1.policy(agent,last_states[agent],ou_noise) for agent in env.agents}\n",
    "        next_states,rewards,done,info=env.step(actions)\n",
    "        \n",
    "             \n",
    "        agent1.sum_rewards={agent: (agent1.sum_rewards[agent]+rewards[agent]) for agent in rewards}\n",
    "        agent1.episode_steps+=1\n",
    "        \n",
    "        agent1.record((last_states,actions,rewards,next_states))\n",
    "        agent1.learn()\n",
    "   \n",
    "        \n",
    "        for agent in rewards:\n",
    "            agent1.update_target(agent1.target_actor_models[agent].variables,agent1.actor_models[agent].variables)\n",
    "        \n",
    "        agent1.update_target(agent1.target_critic.variables,agent1.critic.variables)\n",
    "        \n",
    "        if done['agent_0']:\n",
    "            break\n",
    "        last_states=next_states\n",
    "    \n",
    "    print(\"EPISODE \",episode,\" STEPS = \",agent1.episode_steps)\n",
    "    print(\"REWARD \",agent1.sum_rewards)\n",
    "        \n",
    "    ep_reward_list.append(agent1.sum_rewards)\n",
    "    episode_steps.append(agent1.episode_steps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tensorflow] *",
   "language": "python",
   "name": "conda-env-.conda-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
